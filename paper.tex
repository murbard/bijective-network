\documentclass{article}

\title{Bijective networks}
\author{Arthur Bretitman}
\date{\today}

\usepackage{amsfonts}
\usepackage{amsmath}


\begin{document}

\maketitle

\begin{abstract}
  We introduce bijective networks -- thus named because they form a bijection
  between their input and output space -- as a means of parametrizing
  multivariate probability distributions. We present applications to probability
  density estimation, and variational inference. In these networks, the quantity
  of interest is primarily the determinant of the Jacobian of the network which
  we interpret as a probability density. We also describe a mechanism to
  efficiently train bijective networks.
\end{abstract}

\section{Bijective networks}

\subsection{Definition}

We define a bijective network as a triplet
\((\mathcal{I}, \mathcal{O}, f  : \mathcal{I} \mapsto \mathcal{O})\)
where \(f\) is a diffeomorphism. We use the term ``bijective networks''
because ``diffeomorphic networks'' doesn't have quite the same ring to it,
and the differentiability is generally implied in neural networks. The
requirement can also be weakened to accomodate activation functions like
Relu which are differentiable almost everywhere. Throughput this paper,
differentiability is assumed and we use ``bijective'' and ``diffeomorphic''
interchangeably; we also refer to the determinant of the Jacobian matrix
of that diffeomorphism simply as ``the Jacobian'' and the logarithm of
that determinant as ``the log-Jacobian''.

\subsection{Motivation}

\(f\) can be interpreted as a pullback  measure, with
\(\left|J_f\right| = \frac{d \mathcal{O}}{d \mathcal{I}}\). A rich, parametric,
model of arbitrary probability distribution is a useful building block for many
statistical learning algorithms such as variational inference or density
estimation.

\subsection{Construction}

Concretely, we implement bijective networks as a composition of fully
connected layers of \(n\) neurons each. Note that \(n\) is the dimension of
the input layer as well as that of the output layer. If the problem's
dimensionality is too low and impedes on the expressivity of the network,
it's possible to increase \(n\) by introducing dummy, independent, variables,
and marginalizing over them later.

Hidden layers take their values in \(\mathbb{R}^n\) and the output layer in
\((-1,1)^n\). The input layer can take its value in a variety of domains,
typically products of \(\mathbb{R}\) and \((-1,1)\), with the idea that
\((-1, 1)\) can be mapped to \(\mathbb{R}\) with \(\mathrm{arctanh}\).
In what follows we assume the input layer takes its values in \(\mathbb{R}^n\),
without a loss of generality. For the sake of concretness, we will specify the
non-linear activation functions used in the construction, but other appropriate
functions may of course be substituted.

We suggest using \(\mathrm{arsinh}\) as a close cousin to the sigmoid. Since
its asymptotic behavior is logarithmic it can easily lead to vanishing
gradients, therefore, we alternate with using \(\mathrm{sinh}\) on every other
layer. The final output layer squashes the values to \((-1,1)^n\) by applying
\(\mathrm{tanh}\). All weight matrices are, and remain, invertible. As a
finite composition of diffeomorphisms the network is, itself, a
diffeomorphism.

\subsubsection{Notation}

The input layer is represented as vector \(x\), the \(m-1\) hidden layers as
\(h_i, i \in 2 \ldots m-1 \) the ouput layer is denoted \(y'\). In general,
if \(l\) is a layer, \(l'\) designates the ``activated'' layer, which has been
passed through the activation function.

All of these vectors have dimension \(n\). For ease of notation, we also let
\(h'_0 = x\)

\[
  \left\{
    \begin{aligned}
      h_{i} &= W_{i} \cdot  h'_{i-1} + b_{i} \\
      h'_{2i+1} &= \mathrm{arsinh}(h_{2i+1}) \\
      h'_{2i} &= \mathrm{sinh}(h_{2i}) \\
      y &= W_{m} \cdot h'_m + b_m  \\
      y' &= \mathrm{tanh}(y) \\
    \end{aligned}
  \right.
\]

\subsection{Computing}

\subsubsection{Computing the Jacobian}
The Jacobian  of the network is the product of the Jacobians of each
layer. The Jacobian of the transformation \( u \mapsto W u + b \) is simply
\( W \), the Jacobian of the coordinate-wise non-linear transforms is simply
the diagonal matrix with the derivative of the activation function at the
matching coordinate.

More specifically we are interested in the log of the determinant of the
Jacobian matrix (the log Jacobian). The log Jacobian of the network we defined
is exacly

\[
  J(x) = \sum_{i=1}^{m} \log |W_{i}| +
  \sum_{j=1}^n \left(\log(1-{y'_j}^2)\right)
  + \frac{1}{2} \sum_{i=1}^{m / 2} \sum_{j=1}^n
  \left( \log (1 + (h'_{2i})_j^2) - \log (1 + (h_{2i+1})_j^2) \right)
\]

\subsubsection{The chain rule}
We are interested in taking gradient steps on the log Jacobian with respect to
our parameters, that is the weight matrices and bias vectors. The chain rule
can be used, as in regular neural networks, with the only oddity being that we
need to take derivatives of the log determinant of matrices with respect to
themselves.

A well known identity is

\[
  \frac{d \log |W|}{d W} = (W^{-1})^\top
\]

It looks as if we might need to compute the inverse of the weight matrix, a
costly (\(\mathcal{O}(n^{2.4\ldots})\), for the optimist) operation each time we
compute a gradient step. Fortunately, we can, without loss of generality,
represent \(W\) as the product of \(n(n-1)\) matrices which operate only on two
coordinates. To see why that is the case, consider that the gaussian elimination
algorithm.

Concretely, instead of maintaining a full representation of the matrix \(W\) we
maintain a list of \(n(n-1)\) \(2 \times 2\) matrices. The time complexity of
the matrix vector product remains \(\mathcal{O}(n^2)\), as if multiplying
directly by \(W\), but the log Jacobian and the gradient updates can now be
computed in \(\mathcal{O}(n^2)\). A downside is that the matrix-vector product,
though asymptotically equivalent, will be computed far less efficiently
than if optimized BLAS methods were used. Additionaly, large batch sizes
cannot benefit from Strassen multiplication with this representation.

The gradients can be derived as follow

WARNING THIS IS LIKELY TO BE WRONG, THERE SHOULD BE DIAGONAL MATRICES
IN THERE
\[
  \begin{aligned}
  \frac{d lJ}{d W_m} &= - 2 y' (h'_m)^{\top} + W_m^{-1}\\
  \frac{d lJ}{d b_m} &= - 2 y'\\
  \frac{d lJ}{d W_{i < m}} &= \frac{d lJ}{d h_{i-1}} h'_{i-1} + W_{i}^{-1}\\
\end{aligned}
\]

\section{Application to density estimation}

Assume a sample of \(N\) points, each in \(\mathcal{R}^n\). We are interested in
finding a probability distribution to fit that sample. Since a dirac-comb,
the empirical distribution, will clearly do the job, we need some sort of
regularization. In this case, we try to represent that probability distribution
with a bijective network and the regularization comes from the limited
expressivity of the network and the use of the stochastic-gradient descent
algorithm with early stopping.

We can look at the problem through the length of the minimum description length
principle. Suppose that, to compress the sample, we first pass it through an
invertible function, and then we store the output by truncating the output.

The more we can truncate the output without losing our ability to recover the
input, the better the compression. Therefore, we would like for that function
to blow up its input domain in the regions around the sample points, which
implies contracting it in other regions, since the image of the input domain
is a fixed hypercube. This is equivalent to saying the sum of the log jacobian,
taken over the antecedant of the sample, should be maximized.

We can look at the network as building a copula. The inverse of the Jacobian
(divided by \(2^n\)) is a probability distribution over the input space which
smoothly approximates the empirical distribution.

\section{Application to variational inference}

\subsection{Variational inference}

In this setup, we are given a prior \(P(Z)\) over some latent variables,
a generative model \(P(X | Z)\) and a sample \(\overline{X}\). We would like to
estimate \(P(Z | \overline{X})\). Markov chain Monte Carlo techniques let us
sample from \(P(Z | \overline{X})\), but they are slow, hard to diagnose, and
can suffer from poor convergence.

An popular approximate technique is variational inference. In this model, a
parametric distribution \(Q(Z)\) is optimized to minimize a lower bound
on the KL divergence between \(Q\) and \(P(Z | X)\)

\[
  \int_{Z} Q(Z) \log \frac{Q(Z)}{P(Z, X)}
\]

In practice, \(Q\) has a simple parametric form and factors over variable,
which leads to closed form integration formulas and a fast optimization
algorithm.

However, this often leads \(Q\) to be a rather poor approximation of
\(P(Z | X)\) as the family for \(Q\) isn't flexible enough to represent
the shape of the true posterior.

\subsection{Variational inference, with bijective networks}

The inverse of the Jacobian of a bijective network is a probability
distribution. Its parametric representation is rich enough to fit
complex posteriors. What's more, it is easy to sample from it, by drawing an
output value uniformly at random, and computing its antecedant. To our
knowledge, no other model offers:

\begin{itemize}
\item A rich parametrization capable of representing any probability distribution
\item Efficient sampling
\item Knowlege of the partition function (the constant \(2^n\) in our case)
\end{itemize}

The quantity we are seeking to minimize is

\[
  \mathbb{E}_{Q(Z)} \left(\log \frac{Q(Z)}{P(Z,X)} \right)
\]

Our algorithm thus consists in:

\begin{enumerate}
\item Draw \(U_i\) uniformly at random in the \((-1,1)^n\)
\item Let \(Z_i = f^{-1}(U_i)\) and \(Q(Z_i) = -\log J(f, U_i)\)
\item Let \(\mathcal{L} = \frac{Q(Z_i)}{P(Z_i, X)}\). \(\mathcal{L}\) is an
  unbiased estimate of the variational bound.
\item Take a gradient step to minimize \(\mathcal{L}\)
\end{enumerate}

At a first glance, it might seem that \(P\) doesn't appear in the derivative
of \(\mathcal{L}\) with respect to the network weights, which means the
algorithm would have no chance of working! However, the dependency on
\(P\) appears once we take into account that \(Z_i\) depends the nework weights
(it's essentially the reparametrization trick).


\subsection{Switching the KL divergence}

In fact, since we are not attempting to find closed form formulas for
integrating over \(Q(Z)\), we do not have to use the Variational inference
trick of minimizing \(D_{KL}(Q(Z)||P(Z|\overline{X}))\) and we can tackle
\(D_{KL}(P(Z|\overline{X})||Q(Z))\) which is a more natural metric. We note that

\[
\begin{aligned}
  & D_{KL}(P(Z|\overline{X})|| Q(Z)) \\
  &= \int_Z P(z|\overline{X}) \log \frac{P(z|\overline{X})}{Q(z)} \mathrm{d}z \\
  &= \int_Z Q(z) \frac{P(z|\overline{X})}{Q(z)} \log \frac{P(z|\overline{X})}{Q(z)}  \mathrm{d}z \\
  &= \int_Z Q(z) \frac{P(\overline{X}, z)}{P(\overline{X})Q(z)} \log \frac{P(\overline{X}, z)}{P(\overline{X})Q(z)}  \mathrm{d}z \\
  &= \int_Z Q(z) \frac{P(\overline{X}, z)}{P(\overline{X})Q(z)} \log \frac{P(\overline{X}, z)}{Q(z)}  \mathrm{d}z
  - \int_Z P(z|\overline{X}) \log P(\overline{X})  \mathrm{d}z \\
  &= \frac{1}{P(\overline{X})}\mathbb{E}_{Q} \left( \frac{P(\overline{X}, z)}{Q(z)} \log \frac{P(\overline{X}, z)}{Q(z)}\right) + K
\end{aligned}
\]

We can minimize this metric by repeatedly sampling \(z_i\) from \(Q\),
as before, and minimizing

\[
  \frac{P(\overline{X}, z_i)}{Q(z_i)} \log \frac{P(\overline{X}, z_i)}{Q(z_i)}
\]

instead of \(\log(Q(z_i)/P(z_i, X))\)

TODO: The gradient is 0 for Q(z_i) = e P(z_i) something's weird


\section{Empirical results}

Test this thing


\section{Conclusion}

Bijective neural networks can be trained to fit distributions instead of
functions, and they have a backpropagation algorithm with the same asymptotic
complexity. This opens interesting possibilities for Bayesian inference and
density estimation.

\end{document}